---
title: "Analisi delle Componenti Principali"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1. Caricare i dati `marks` 

```{r}
marks <- read.table("http://www.maths.leeds.ac.uk/~charles/mva-data/openclosedbook.dat",header = TRUE)
X = as.matrix(marks)
# assegno i nomi alle variabili
colnames(X) <- c("Mechanics", "Vectors", "Algebra", "Analysis", "Statistics")
# guardo le prime righe 
head(X)
n = nrow(X)
p = ncol(X)
```

2. Calcolare la matrice dei dati centrati $\tilde{X}$ come trasformazione lineare $\underset{n\times p}{X}\underset{p\times q}{A'} + 
\underset{n\times 1}{1}\underset{1\times q}{b'}$ con $q=p$, $A = \underset{p\times p}{I}$ e $b = -\underset{p\times 1}{\bar{x}}$

```{r}
A = diag(rep(1,p))
one.n = matrix(rep(1,n))
b = (1/n) * t(X) %*% one.n

Xtilde = X %*% t(A) + one.n %*% (-t(b))
```

2. Calcolare il voto medio di ciascun studente come combinazione lineare $\underset{n\times 1}{y} =  \underset{n\times p}{X}\underset{p\times 1}{a}$ con $a_j = 1/p$, $j=1,\ldots,p$ 

```{r}
a = matrix(rep(1/p, p), ncol=1)
y = X %*% a
```

2. Calcolare la prima componente principale di $\tilde{X}$ come  $\underset{n\times 1}{y_1} = \underset{n\times p}{\tilde{X}} \underset{p\times 1}{v_1}$ dove $v_1$ è il primo autovettore di $S=\frac{1}{n} \tilde{X}' \tilde{X}$ associato all'autovalore più grande $\lambda_1$. Verificare che la varianza di $y_1$ è pari a $\lambda_1$ e che è maggiore della varianza della combinazione lineare normalizzata $\underset{n\times 1}{y} = \underset{n\times p}{\tilde{X}}\underset{p\times 1}{a}$ con $a_j = 1/\sqrt{p}$, $j=1,\ldots,p$. 

```{r}
# decomposizione spettrale di S
S = (1/n) * t(Xtilde) %*% Xtilde
eigen = eigen(S)
Lambda = diag(eigen$values)
V = eigen$vectors

# pesi (loadings) della 1ma componente principale 
v1 = V[,1, drop=FALSE]
v1

# punteggi (scores) della 1ma componente principale
y1 = Xtilde %*% v1

# varianza di y1
var(y1) * (n-1)/n # coincide con Lambda[1,1]

# confronto con altra combinazione lineare normalizzata
a = matrix(rep(1/sqrt(p), p), ncol=1)
y = Xtilde %*% a 
var(y) * (n-1)/n
```

3. Calcolare le $p$ componenti principali $\underset{n\times p}{Y} =  \underset{n\times p}{\tilde{X}} \underset{p\times p}{V}$. Verificare che il vettore medio di $Y$ è nullo, la matrice di varianze/covarianze $S^Y$ di $Y$ è pari a $\Lambda$, che la varianza totale e generalizzata di $S^Y$ è pari a quella di $S$

```{r}
# p componenti principali Y
Y = Xtilde %*% V

# vettore medio di Y
round(
(1/n) * t(Y) %*% one.n
, 8)

# matrice di varianze covarianze di Y
S_Y = (1/n) * t(Y) %*% Y # coincide con Lambda
round(
S_Y
, 8)

# varianza totale di S_Y
sum(diag(S_Y)) # coincide con sum(diag(S_Y))

# varianza generalizzata di S_Y
det(S_Y) # coincide con det(S)
```

4. Calcolare le componenti principali con il comando `princomp()`
e `prcomp()`

```{r}
pca = princomp(X)
summary(pca) # Standard deviation coincide con sqrt(diag(Lambda))

# pesi
pca$loadings[,] # coincide con V

# punteggi
head( pca$scores ) # coincide con head(Y)


pca = prcomp(X, center = TRUE)
summary(pca)  # Standard deviation coincide con sqrt(diag(Lambda)*(n/n-1))

# pesi
pca$rotation[,] # coincide con V

# punteggi
head( pca$x ) # coincide con head(Y)
```
