---
title: '**Analisi delle Componenti Principali**'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval= T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```

# Dati marks

1. Caricare i dati `marks` 

```{r}
marks <- read.table("http://www.maths.leeds.ac.uk/~charles/mva-data/openclosedbook.dat",header = TRUE)
X = as.matrix(marks)
# assegno i nomi alle variabili
colnames(X) <- c("Mechanics", "Vectors", "Algebra", "Analysis", "Statistics")
# guardo le prime righe 
head(X)
n = nrow(X)
p = ncol(X)
```

2. Calcolare la matrice dei dati centrati $\tilde{X}$ come trasformazione lineare $\underset{n\times p}{X}\underset{p\times q}{A'} + 
\underset{n\times 1}{1}\underset{1\times q}{b'}$ con $q=p$, $A = \underset{p\times p}{I}$ e $b = -\underset{p\times 1}{\bar{x}}$

```{r}
A = diag(rep(1,p))
one.n = matrix(rep(1,n))
b = (1/n) * t(X) %*% one.n

Xtilde = X %*% t(A) + one.n %*% (-t(b))
```

2. Calcolare il voto medio di ciascun studente come combinazione lineare $\underset{n\times 1}{y} =  \underset{n\times p}{X}\underset{p\times 1}{a}$ con $a_j = 1/p$, $j=1,\ldots,p$ 

```{r}
a = matrix(rep(1/p, p), ncol=1)
y = X %*% a
```

2. Calcolare la prima componente principale di $\tilde{X}$ come  $\underset{n\times 1}{y_1} = \underset{n\times p}{\tilde{X}} \underset{p\times 1}{v_1}$ dove $v_1$ è il primo autovettore di $S=\frac{1}{n} \tilde{X}' \tilde{X}$ associato all'autovalore più grande $\lambda_1$. Verificare che la varianza di $y_1$ è pari a $\lambda_1$ e che è maggiore della varianza della combinazione lineare normalizzata $\underset{n\times 1}{y} = \underset{n\times p}{\tilde{X}}\underset{p\times 1}{a}$ con $a_j = 1/\sqrt{p}$, $j=1,\ldots,p$. 

```{r}
# decomposizione spettrale di S
S = (1/n) * t(Xtilde) %*% Xtilde
eigen = eigen(S)
Lambda = diag(eigen$values)
V = eigen$vectors

# pesi (loadings) della 1ma componente principale 
v1 = V[,1, drop=FALSE]
v1

# punteggi (scores) della 1ma componente principale
y1 = Xtilde %*% v1

# varianza di y1
var(y1) * (n-1)/n # coincide con Lambda[1,1]

# confronto con altra combinazione lineare normalizzata
a = matrix(rep(1/sqrt(p), p), ncol=1)
y = Xtilde %*% a 
var(y) * (n-1)/n
```

3. Calcolare le $p$ componenti principali $\underset{n\times p}{Y} =  \underset{n\times p}{\tilde{X}} \underset{p\times p}{V}$. Verificare che il vettore medio di $Y$ è nullo, la matrice di varianze/covarianze $S^Y$ di $Y$ è pari a $\Lambda$, che la varianza totale e generalizzata di $S^Y$ è pari a quella di $S$

```{r}
# p componenti principali Y
Y = Xtilde %*% V

# vettore medio di Y
round(
(1/n) * t(Y) %*% one.n
, 8)

# matrice di varianze covarianze di Y
S_Y = (1/n) * t(Y) %*% Y # coincide con Lambda
round(
S_Y
, 8)

# varianza totale di S_Y
sum(diag(S_Y)) # coincide con sum(diag(S_Y))

# varianza generalizzata di S_Y
det(S_Y) # coincide con det(S)
```

4. Calcolare le componenti principali con il comando `princomp()`
e `prcomp()`

```{r}
pca = princomp(X)
summary(pca) # Standard deviation coincide con sqrt(diag(Lambda))

# pesi
pca$loadings[,] # coincide con V

# punteggi
head( pca$scores ) # coincide con head(Y)


pca = prcomp(X, center = TRUE)
summary(pca)  # Standard deviation coincide con sqrt(diag(Lambda)*(n/n-1))

# pesi
pca$rotation[,] # coincide con V

# punteggi
head( pca$x ) # coincide con head(Y)
```

5. Costruire il *biplot* e commentare. 

```{r}
# biplot
biplot(pca)
```

6. Calcolare la correlazione tra il voto centrato sullo 0 dell'esame in `Statistics` e i punteggi della prima componente principale

```{r}
V[5,1]*pca$sdev[1]/sqrt(diag(S)[5])
cor(Xtilde[,"Statistics"],Y[,1])
```

7. Scegliere il numero $q$ di componenti principali utilizzando i 
criteri (i) proporzione di varianza spiegata dalle prime $q$ componenti superiore all'80\%; (ii) varianza spiegata da ciascuna componente maggiore di $c=\frac{1}{p}\sum_{j=1}^{p}\lambda_j$ (iii) utilizzando lo *scree plot*.

```{r}
# scelta di q: proporzione di varianza spiegata > 80%
summary(pca)

# scelta di q: varianza spiegata > c
c = mean(pca$sdev^2)
pca$sdev^2 > c

# scelta di q: scree plot
plot(pca, type="line")
```

# Dati PES

1. Importare i dati `EURO4PlayerSkillsSep11` dalla libreria `SportsAnalytics`

```{r}
rm(list=ls())
# carico la libreria SportsAnalytics (da installare)
library(SportsAnalytics)
data("EURO4PlayerSkillsSep11")
PES = EURO4PlayerSkillsSep11
```

2. Selezionare solo i giocatori del campionato italiano e le variabili corrispondenti alle colonne da 15 a 40

```{r}
gita = PES$Nationality=="Italian"
X = as.matrix(PES[gita,15:40])
Position = as.factor(PES$Position[gita])
Name = PES$Name[gita]
```

3. Applicare l'analisi delle componenti principali. Costruire il biplot con la funzione `autoplot` della libreria `ggfortify`, 
colorando ogni punto in corrispondenza della posizione del giocatore in campo.

```{r}
pca = prcomp(X, center=TRUE)
library(ggfortify)
XPos = data.frame(X,Position)
autoplot(pca, data = XPos, colour = "Position", 
         loadings=TRUE, loadings.label = TRUE) + theme_bw()
```

