---
title: '**Analisi delle Componenti Principali**'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval= T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```

# Dati Marks

1. Caricare i dati `marks` 

```{r}
marks <- read.table("http://www.maths.leeds.ac.uk/~charles/mva-data/openclosedbook.dat",header = TRUE)
X = as.matrix(marks)
# assegno i nomi alle variabili
colnames(X) <- c("Mechanics", "Vectors", "Algebra", "Analysis", "Statistics")
# guardo le prime righe 
head(X)
n = nrow(X)
p = ncol(X)
```

2. Calcolare la matrice dei dati centrati $\tilde{X}$ come trasformazione lineare $\underset{n\times p}{X}\underset{p\times q}{A'} + 
\underset{n\times 1}{1}\underset{1\times q}{b'}$ con $q=p$, $A = \underset{p\times p}{I}$ e $b = -\underset{p\times 1}{\bar{x}}$

```{r}
A = diag(rep(1,p))
one.n = matrix(rep(1,n))
b = (1/n) * t(X) %*% one.n

Xtilde = X %*% t(A) + one.n %*% (-t(b))
```

2. Calcolare il voto medio di ciascun studente come combinazione lineare $\underset{n\times 1}{y} =  \underset{n\times p}{X}\underset{p\times 1}{a}$ con $a_j = 1/p$, $j=1,\ldots,p$ 

```{r}
a = matrix(rep(1/p, p), ncol=1)
y = X %*% a
```

2. Calcolare la prima componente principale di $\tilde{X}$ come  $\underset{n\times 1}{y_1} = \underset{n\times p}{\tilde{X}} \underset{p\times 1}{v_1}$ dove $v_1$ è il primo autovettore di $S=\frac{1}{n} \tilde{X}' \tilde{X}$ associato all'autovalore più grande $\lambda_1$. Verificare che la varianza di $y_1$ è pari a $\lambda_1$ e che è maggiore della varianza della combinazione lineare normalizzata $\underset{n\times 1}{y} = \underset{n\times p}{\tilde{X}}\underset{p\times 1}{a}$ con $a_j = 1/\sqrt{p}$, $j=1,\ldots,p$. 

```{r}
# decomposizione spettrale di S
S = (1/n) * t(Xtilde) %*% Xtilde
eigen = eigen(S)
Lambda = diag(eigen$values)
V = eigen$vectors

# pesi (loadings) della 1ma componente principale 
v1 = V[,1, drop=FALSE]
v1

# punteggi (scores) della 1ma componente principale
y1 = Xtilde %*% v1

# varianza di y1
var(y1) * (n-1)/n # coincide con Lambda[1,1]

# confronto con altra combinazione lineare normalizzata
a = matrix(rep(1/sqrt(p), p), ncol=1)
y = Xtilde %*% a 
var(y) * (n-1)/n
```

3. Calcolare le $p$ componenti principali $\underset{n\times p}{Y} =  \underset{n\times p}{\tilde{X}} \underset{p\times p}{V}$. Verificare che il vettore medio di $Y$ è nullo, la matrice di varianze/covarianze $S^Y$ di $Y$ è pari a $\Lambda$, che la varianza totale e generalizzata di $S^Y$ è pari a quella di $S$

```{r}
# p componenti principali Y
Y = Xtilde %*% V

# vettore medio di Y
round(
(1/n) * t(Y) %*% one.n
, 8)

# matrice di varianze covarianze di Y
S_Y = (1/n) * t(Y) %*% Y # coincide con Lambda
round(
S_Y
, 8)

# varianza totale di S_Y
sum(diag(S_Y)) # coincide con sum(diag(S_Y))

# varianza generalizzata di S_Y
det(S_Y) # coincide con det(S)
```

4. Calcolare le componenti principali con il comando `princomp()`
e `prcomp()`

```{r}
pca = princomp(X)
summary(pca) # Standard deviation coincide con sqrt(diag(Lambda))

# pesi
pca$loadings[,] # coincide con V

# punteggi
head( pca$scores ) # coincide con head(Y)


pca2 = prcomp(X, center = TRUE)
summary(pca2)  # Standard deviation coincide con sqrt(diag(Lambda)*(n/n-1))

# pesi
pca2$rotation[,] # coincide con V

# punteggi
head( pca2$x ) # coincide con head(Y)
```

5. Costruire il *biplot* e commentare. 

```{r}
# biplot
biplot(pca)
```

6. Calcolare la correlazione tra il voto centrato sullo 0 dell'esame in `Statistics` (quinta colonna della matrice $\tilde{X}$) e i punteggi della prima componente principale (prima colonna della matrice $Y$). Si ricordi la formula
$$\frac{v_{jk}\sqrt{\lambda_k}}{\sqrt{s_{jj}}}$$ per la $j$-sima colonna di $\tilde{X}$ e la $k$-sima colonna di $Y$.

```{r}
V[5,1]*pca$sdev[1]/sqrt(diag(S)[5])
# corrisponde alla correlazione calcolata sulla base dei dati
cor(Xtilde[,"Statistics"],Y[,1])
```

7. Scegliere il numero $q$ di componenti principali utilizzando i 
criteri (i) proporzione di varianza spiegata dalle prime $q$ componenti superiore all'80\%; (ii) varianza spiegata da ciascuna componente maggiore di $c=\frac{1}{p}\sum_{j=1}^{p}\lambda_j$ (iii) utilizzando lo *scree plot*.

```{r}
# scelta di q: proporzione di varianza spiegata > 80%
summary(pca)

# scelta di q: varianza spiegata > c
c = mean(pca$sdev^2)
pca$sdev^2 > c

# scelta di q: scree plot
plot(pca, type="line")
```


# Dati Wine

1. Importare i dati ed effettuare l'analisi delle componenti principali dei dati standardizzati $\underset{n\times p}{Z}$ (escludendo la variabile `type`), calcolando la matrice dei 
punteggi $\underset{n\times p}{Y} = \underset{n\times p}{X}\underset{p\times p}{Z}$ e la matrice dei 
pesi $\underset{p\times p}{V}$. Calcolare la percentuale di varianza spiegata dalle prime $q$ componenti principali, per $q=1,\ldots,10$. 


```{r}
rm(list=ls())
wine <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", header  = FALSE)
colnames(wine)<-c("Type","Alcohol",
                  "Malic","Ash",
                  "Alcalinity","Magnesium",
                  "Phenols","Flavanoids",
                  "Nonflavanoids","Proanthocyanins",
                  "Color_int","Hue","Dilution","Proline")
wine$Type = factor(wine$Type)
X = as.matrix(wine[,-1])
n = nrow(X)
p = ncol(X)

# PCA
pca = princomp(X, cor=T)

V = pca$loadings
Y = pca$scores

# prop. di varianza spiegata dalle prime q cp
q <- 10
cumsum(pca$sdev^2/sum(pca$sdev^2))[1:q]
```

2. Costruire il diagramma di dispersione dei punteggi delle prime due componenti principali, colorando i punti con colori diversi a seconda della tipologia di vino (variabile `type`). 

```{r}
plot(Y[,1:2], col=as.numeric(wine$Type), asp=1)
```

3.  Costruire il biplot con la funzione `autoplot` della libreria `ggfortify`.

```{r}
library(ggfortify)
autoplot(pca, data = wine, colour = "Type", 
         loadings=TRUE, loadings.label = TRUE) + theme_bw()
```


# Dati Face

1. Importare i dati e visualizzare l'immagine con il comando `image`.  Effettuare l'analisi delle componenti 
principali dei dati centrati $\underset{n\times p}{\tilde{X}}$, calcolando la matrice dei 
punteggi $\underset{n\times p}{Y} = \underset{n\times p}{\tilde{X}}\underset{p\times p}{V}$ e la matrice dei 
pesi $\underset{p\times p}{V}$.


```{r}
rm(list=ls())
face <- read.table("https://raw.githubusercontent.com/aldosolari/AE/master/docs/dati/face.txt", header=FALSE)

X = as.matrix(face)
n = nrow(face)
p = ncol(face)

# visualizza immagine
image(X, col=gray(0:255/255), asp=p/n)

# PCA
pca = princomp(X, cor=F)
V = pca$loadings
Y = pca$scores
xbar = matrix(pca$center, ncol=1)
```

2. Ottenere la migliore approssimazione per $\underset{n\times p}{\tilde{X}}$ di rango $q$, 
$\underset{n\times p}{A_q} = \underset{n\times q}{Y_q} \underset{q\times p}{V_q'}$, con $q=10$. 
Costruire l'immagine compressa $\underset{n\times p}{C} = \underset{n\times p}{A_q} + \underset{n\times 1}{1}\underset{1\times p}{\bar{x}}$, assicurandosi che tutti gli elementi di $\underset{n\times p}{C}$ siano compresi tra 0 e 1. 

```{r}
q = 10
Yq = Y[,1:q]
Vq = V[,1:q]

# migliore approssimazione di rango q
Aq = Yq %*% t(Vq)

# compressione immagine
one.n = matrix(rep(1,n), ncol=1)
face2 = Aq + one.n %*% t(xbar)

# forzo i valori tra 0 e 1
face2 <- pmax(pmin(face2, 1), 0)
```

3. Visualizzare l'immagine compressa e confrontarla con l'immagine originale calcolando il fattore di riduzione in termini di pixels e bytes (utilizzando il comando  `object.size`) 

```{r}
# visualizza immagine compressa
image(face2, col=gray(0:255/255), asp=p/n)

# salve immagine compressa
# library(png)
# writePNG(face,"face.png")

# confronta pixels utilizzati

pixels = prod(dim(face))
pixels2 = prod(dim(Yq)) + prod(dim(Vq)) + prod(dim(xbar))
round(pixels/pixels2, 2) # fattore di riduzione

# confronta memoria utilizzata
size = object.size(face)
size2 = object.size(Yq) + object.size(Vq) + object.size(xbar)
round( size/size2, 2) # fattore di riduzione
```

<!-- # Dati PES -->

<!-- 1. Importare i dati `EURO4PlayerSkillsSep11` dalla libreria `SportsAnalytics` -->

<!-- ```{r} -->
<!-- rm(list=ls()) -->
<!-- # carico la libreria SportsAnalytics (da installare) -->
<!-- library(SportsAnalytics) -->
<!-- data("EURO4PlayerSkillsSep11") -->
<!-- PES = EURO4PlayerSkillsSep11 -->
<!-- ``` -->

<!-- 2. Selezionare solo i giocatori del campionato italiano e le variabili corrispondenti alle colonne da 15 a 40 -->

<!-- ```{r} -->
<!-- gita = PES$Nationality=="Italian" -->
<!-- X = as.matrix(PES[gita,15:40]) -->
<!-- Position = as.factor(PES$Position[gita]) -->
<!-- Name = PES$Name[gita] -->
<!-- ``` -->

<!-- 3. Applicare l'analisi delle componenti principali. Costruire il biplot con la funzione `autoplot` della libreria `ggfortify`,  -->
<!-- colorando ogni punto in corrispondenza della posizione del giocatore in campo. -->

<!-- ```{r} -->
<!-- pca = prcomp(X, center=TRUE) -->
<!-- library(ggfortify) -->
<!-- XPos = data.frame(X,Position) -->
<!-- autoplot(pca, data = XPos, colour = "Position",  -->
<!--          loadings=TRUE, loadings.label = TRUE) + theme_bw() -->
<!-- ``` -->

