---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Cognome: $\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots$ Nome: $\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots$ Matricola: $\ldots\ldots\ldots\ldots\ldots\ldots\ldots$ 

Tipologia d'esame: $\Box$ 12 CFU $\qquad$ $\Box$ 15 CFU 

## Prova scritta di ASM 12CFU e 15CFU - Modulo Analisi Esplorativa del 27.06.2017

*La durata della prova è di 70 minuti*.\
*Si svolgano gli esercizi 1, 2 e 3 riportando il risultato dove indicato.*


### Esercizio 1 (Punti: 10)

Si consideri la seguente matrice di varianze/covarianze $\underset{3\times 3}{S} = 
\left[
\begin{array}{ccc}
4 & 0 & 0 \\
0 & 9 & 0 \\
0 & 0 & 1 \\
\end{array}\right].$

1.a) Riportare l'indice di variabilità relativo (\underline{arrotondare al secondo decimale})  


$$=\ldots\ldots\ldots\ldots.$$
    
1.b) Determinare la matrice di correlazione

$\underset{3\times 3}{R} = 
\left[
\begin{array}{ccc}
\\
\ldots\ldots &  \ldots\ldots  &  \ldots\ldots \\
\\
\ldots\ldots &  \ldots\ldots  &  \ldots\ldots\\
\\
\ldots\ldots &  \ldots\ldots  &  \ldots\ldots\\
\end{array}\right].$

1.c) Sia $\underset{n\times 3}{\tilde{X}}$ la matrice dei dati centrati a cui corrisponde la matrice di correlazione $\underset{3\times 3}{R}$ calcolata al punto precedente. Determinare l'angolo (espresso in gradi) tra $\underset{n\times 1}{\tilde{x}_1}$ e $\underset{n\times 1}{\tilde{x}_2}$:

$$=\ldots\ldots\ldots\ldots$$ 

1.d) Determinare gli autovalori $\lambda_1$, $\lambda_2$ e $\lambda_3$ associati alla matrice di varianze/covarianze $\underset{3\times 3}{S}$  (\underline{arrotondare al secondo decimale})  

$$\lambda_1=\ldots\ldots, \quad \lambda_2=\ldots\ldots, \quad \lambda_3=\ldots\ldots$$

1.e) Riportare la proporzione di varianza spiegata dalle prime due componenti principali calcolate a partire dalla matrice di correlazione $\underset{3\times 3}{S}$   (\underline{arrotondare al secondo decimale})  

$$=\ldots\ldots\ldots\ldots$$

1.f) *Dimostrazione*

Dimostrare che la matrice di centramento $\underset{n\times n}{H}$ è idempotente, giustificano tutti i passaggi. 

\fbox{\parbox[b][12em][t]{1\textwidth}{ 

\bigskip
$\quad$

}}




```{r, echo=F, include=T}
rm(list=ls())
S = matrix(c(4,0,0, 0, 9, 0, 0, 0, 1), ncol=3)
# 1.a
round( det(S)/prod(diag(S)), 2)
# 2.b
R = diag(c(4,9,1)^(-1/2)) %*% S  %*% diag(c(4,9,1)^(-1/2))
R
# 2.c 
acos(R[1,2])*(180/pi)
# 2.d
lambdas = eigen(S)$values
round( lambdas , 2)

# 2.e
round( sum(lambdas[1:2])/sum(lambdas) , 2)
```

\newpage

### Esercizio 2 (Punti: 8)

Si consideri la seguente matrice dei dati relativa a 4 unità statistiche

$$
\underset{4\times 2}{X} = \left[
\begin{array}{cc}
2.5 & 2.5\\
4.5 & 8.5\\
6.5 & 1.5\\
3.5 & 3.5 \\
\end{array}
\right] 
$$

a. Si calcoli la matrice delle distanze $\underset{4\times 4}{D}$ per la matrice $\underset{4\times 2}{X}$ riportata sopra utilizzando la metrica di Manhattan, riportando la definizione di distanza di Manhattan $d_{1}(u_i,u_l)$ tra due unità statistiche $\underset{1\times p}{u'_i}$ e $\underset{1\times p}{u'_l}$ per una generica matrice di dati $\underset{n\times p}{X}$.

\fbox{\parbox[b][8em][t]{1\textwidth}{ 
\bigskip
$
\underset{4\times 4}{D} = 
\left[
\begin{array}{cccc}
\smallskip
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$\\
\smallskip
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$\\
\smallskip
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$\\
\smallskip
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$
\smallskip
\end{array}
\right] 
$ $\quad$ $d_{1}(u_i,u_l) = $


}}

```{r, echo=F, include=T}
rm(list=ls())
X = matrix(
  c(2.5,2.5,
    4.5,8.5,
    6.5,1.5,
    3.5,3.5), byrow = T, ncol=2
)
row.names(X) = paste("u",1:4, sep="")
( D = dist(X, "manhattan") )
```

b. Utilizzando il metodo del legame completo con riferimento alla matrice delle distanze $\underset{4\times 4}{D}$ calcolata al punto a., si riportino i gruppi (*clusters*) identificati tagliando il dendogramma ad altezza $c=6$.

\fbox{\parbox[b][5em][t]{1\textwidth}{ 
$\quad$
}}

```{r, echo=F, include=T}
hc = hclust(D, method = "complete")
cutree(hc,h=6)
```



c. Con riferimento a due gruppi (*clusters*) di osservazioni $G_I$ e $G_L$, 
si riporti la definizione di distanza tra i due gruppi secondo il metodo del legame medio.

\fbox{\parbox[b][7em][t]{1\textwidth}{ 
\bigskip
\bigskip
$d_{medio}(G_I,G_L) = $
\bigskip

}}





d. Si completi la seguente tabella:

\begin{tabular}{| lllll |}
\hline
& &  Trasformazioni & Interpretazione &  \\
Legame & Inversione  & monotone & taglio &  \\
 & (Si/No)  & (Invariante/Non invariante) & (Si/No) & \\
\hline
\\
Singolo &  \\
\\
Completo &  \\
\\
Medio &  \\
\\
Centroide &  \\
\\
\hline
\end{tabular}



### Esercizio 3 (Punti: 8)


Si consideri il dataset `mtcars` presente nella libreria `datasets`, che contiene $n=32$ unità statistiche (automobili) relative alle seguenti $11$ variabili:

* *mpg* Miles/(US) gallon
* *cyl* Number of cylinders
* *disp* Displacement (cu.in.)
* *hp* Gross horsepower 
* *drat* Rear axle ratio
* *wt* Weight (1000 lbs)
* *qsec* 1/4 mile time
* *vs* V/S
* *am* Transmission (0 = automatic, 1 = manual)
* *gear* Number of forward gears
* *carb* Number of carburetors


3.a) Si consideri la matrice $\underset{32\times 6}{X}$ che contiene solo le seguenti 6 variabili: *mpg*, *disp*, *hp*, *drat*, *wt* e *qsec*. Per ciascuna unità statistica, si calcoli la distanza di Mahalanobis dal baricentro e si riporti il nome delle 4 marche di automobili con distanza di Mahalanobis inferiore a $1.5$:


\bigskip
$\ldots$

\bigskip
$\ldots$

\bigskip
$\ldots$

\bigskip
$\ldots$

\bigskip




```{r, echo=F, include=T}
rm(list=ls())
X = as.matrix(mtcars[,c(1,3,4,5,6,7)])
n = nrow(X)
# vettore medie
xbar = matrix(colMeans(X), ncol=1)
S = var(X)*((n-1)/n)
# matrice inversa
InvS = solve(S)
# quadrato della distanza di Mahalanobis per le n osservazioni
dM2 = apply(X,1, function(u) t(u-xbar) %*% InvS %*% (u - xbar) )
which(sqrt(dM2) < 1.5)
```




3.b) Partendo da $\underset{32\times 6}{X}$, calcolare la matrice dei dati standardizzati $\underset{32\times 6}{Z}$. 
Calcolare l’indice di Calinski and Harabasz (CH) per un numero di gruppi $K$ da 2 a 8, impostando per ciascun valore di $K$ `set.seed(123)` prima di eseguire l’algoritmo delle K-medie (specificando `algorithm = Lloyd`). Riportare per ciascun valore di $K$ il rispettivo valore dell’indice CH (arrotondando al secondo decimale). 

\bigskip

\begin{tabular}{c|cccccccc}
 $K$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
 \\
 Indice CH & $\ldots\ldots\ldots$ & $\ldots\ldots\ldots$ & $\ldots\ldots\ldots$ & $\ldots\ldots\ldots$ & $\ldots\ldots\ldots$ 
 & $\ldots\ldots\ldots$  & $\ldots\ldots\ldots$ \\
\end{tabular}

```{r, echo=F, include=T}
Z = scale(X, center=T, scale= diag(S)^(1/2))
KS = 2:8
CH <- vector()

for (i in 1:length(KS)){
  K = KS[i]
  set.seed(123)
  km = kmeans(Z, centers = K, algorithm = "Lloyd")
  W = km$tot.withinss
  B = km$betweenss 
  CH[i] = (B/(K-1)) / (W/(n-K))
}

rbind(KS, round(CH,2))
```

\bigskip

3.c) Sulla base di $\underset{32\times 6}{Z}$, calcolare la matrice delle distanze $\underset{32\times 32}{D}$ utilizzando la metrica Euclidea, ed effettuare l'analisi dei cluster gerarchica utilizzando il legame singolo, ricavandone 3 gruppi. 
Calcolare, arrotondando al secondo decimale, il valore medio della silhouette per i tre gruppi individuati (utilizzando il comando `silhouette` presente nella libreria `cluster`). 

```{r, echo=F, include=T}
# valore medio della silhouette
require(cluster)
D = dist(Z, method="euclidean")
hc <- hclust(D, method="single")
gruppi <- cutree(hc, k=3)
sil = silhouette(gruppi, dist=D)
round( summary(sil)$ clus.avg.widths, 2)
```


\bigskip

\begin{tabular}{c|c}
 & Valore medio della Silhouette\\
\hline
Gruppo 1 &  \\
&  \\
Gruppo 2 &  \\
&  \\
Gruppo 3 &  \\
&  \\
 \hline
\end{tabular}






